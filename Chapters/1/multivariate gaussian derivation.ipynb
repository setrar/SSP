{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc60221a-a09b-4c92-a08c-66c58fcda3c8",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4920f-c733-460c-a235-92bfa3252494",
   "metadata": {},
   "source": [
    " - [ ]  _goal:_ derive multivariate Gaussian distribution from univariate Gaussian distribution and two postulates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d4e0f-187e-4579-88cd-12ffcb480602",
   "metadata": {},
   "source": [
    "$$ \n",
    "{\\color{YellowGreen}X} = [x_1 \\dots x_m]^T \\text{ real random vector with specified mean and covariance matrix:} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3998f4a-86b5-4339-ae84-4578a3c9a7d0",
   "metadata": {},
   "source": [
    "- with mean:\n",
    "\n",
    "$$\n",
    "m_X = E{\\color{YellowGreen}X} = [m_{x_1} \\dots m_{x_m}]^T  \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f8d1e-0f0b-4efe-b8e5-0a45b9c42eb2",
   "metadata": {},
   "source": [
    "- and covariance matrix: (centered)\n",
    "$$\n",
    "\\begin{gather}\n",
    "C_{XX} = E({\\color{YellowGreen}X}-m_X)({\\color{YellowGreen}X}-m_X)^T = \\Big[ E({\\color{YellowGreen}X}-m_X)({\\color{YellowGreen}X}-m_X)^T \\Big]_{i,j=1}^m\n",
    "\\\\\n",
    "=\n",
    "E({\\color{YellowGreen}XX}^T-{\\color{YellowGreen}X}m_X^T-m_X{\\color{YellowGreen}X}^T+m_Xm_X^T) \\qquad \\qquad \\quad\n",
    "\\\\\n",
    "= \n",
    "(E{\\color{YellowGreen}X}{\\color{YellowGreen}X}^T) - (E{\\color{YellowGreen}X})m_X^T-m_X(E{\\color{YellowGreen}X})^T + m_Xm_X^T \\qquad\n",
    "\\\\\n",
    "\\qquad = R_{XX}-m_Xm_X^T-\\not{m_Xm_X^T}+\\not{m_Xm_X^T} = R_{XX}-m_Xm_X^T\n",
    "\\end{gather}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fce052-2de3-48a7-ae03-85170cd07d25",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{gather}\n",
    "R_{XX} = E{\\color{YellowGreen}X}{\\color{YellowGreen}X}^T = \\text{ correlation matrix } \\mathit{(not \\space centered)} \\qquad \\qquad \\quad\n",
    "\\\\\n",
    "\\text{ linearity of expectation } \\mathit{E} \\text { exploited } \\mathit{(linear \\space operations \\space commute)}\n",
    "\\end{gather}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb6074-46fa-499e-bb6e-10e29223dd00",
   "metadata": {},
   "source": [
    "- by definition of expectation\n",
    "$$ \n",
    "C_{XX} = E({\\color{YellowGreen}X}-m_X)({\\color{YellowGreen}X}-m_X)^T = \\int \\mathrm{d}x_1 \\dots \\int \\mathrm{d}x_m f_{\\color{YellowGreen}X}(X) ({\\color{YellowGreen}X} - m_x)({\\color{YellowGreen}X} - m_x)^T\n",
    "\\implies C_{XX} = C_{XX}^T \\text { symmetric }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbf6b9-879a-4252-93aa-cd28d006657b",
   "metadata": {},
   "source": [
    "- _Postulate_ that a linear transformation of jointly Gaussian random variables pro-\n",
    "duces again jointly Gaussian random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215b258-3e57-415c-9924-2afb42fe84f5",
   "metadata": {},
   "source": [
    "- Consider now a linear transformation of variables:\n",
    "$$ \n",
    "\\begin{gather}\n",
    "{\\color{Cyan}Z} = V^T({\\color{YellowGreen}X} - m_X)\n",
    "\\\\\n",
    "\\text { Components of } {\\color{Cyan}Z} \\text{ jointly Gaussian }\n",
    "\\\\\n",
    "{\\color{YellowGreen}X} \\text{ is Gaussian } \\iff {\\color{Cyan}Z} \\text { is Gaussian }\n",
    "\\end{gather}\n",
    "$$\n",
    "$$ \n",
    "\\begin{gather}\n",
    "\\text{ Then we find the first two moments: } \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "\\\\\n",
    "m_Z = E \\; V^T({\\color{YellowGreen}X} - m_X ) = V^T(E{\\color{YellowGreen}X} - m_X ) = V^T(m_X - m_X) = 0 \\qquad \\qquad \\qquad\n",
    "\\\\\n",
    "C_{ZZ} = E({\\color{Cyan}Z} - m_Z)({\\color{Cyan}Z} - m_Z)^T = E{\\color{Cyan}Z}{\\color{Cyan}Z}^T = EV^T ({\\color{YellowGreen}X} - m_X)({\\color{YellowGreen}X} - m_X)^TV \\qquad\n",
    "\\\\\n",
    "= \n",
    "V^T \\big( E({\\color{YellowGreen}X} - m_X)({\\color{YellowGreen}X} - m_X)^T \\big) V = V^TC_{XX}V = V^TV \\Lambda V^TV = \\Lambda\n",
    "\\\\\n",
    "\\qquad \\qquad  \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\widehat { \\mathit{eigendecomposition} }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\text{ Or hence } \\mathit{E}_{ {\\color{Cyan}z}_i{\\color{Cyan}z}_j } = \\lambda_i \\sigma_{ij} \\text{ : the } {\\color{Cyan}z}_i \\text{ are zero mean and uncorrelated } \\qquad \\qquad\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d798098-513f-4a90-adb5-6d1e9f566995",
   "metadata": {},
   "source": [
    "- At this point, only are specified :\n",
    "$$ \n",
    "\\text { the first two moments of the } {\\color{Cyan}z}_i. \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\text { Now, specify the rest of their distribution by stating that the } {\\color{Cyan}z}_i \\text { are jointly Gaussian.} \n",
    "\\\\\n",
    "\\text { We furthermore postulate that zero mean uncorrelated Gaussian random variables are independent. }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text { Note that in general } \\quad\n",
    "\\left.\\!\n",
    "    \\begin{array}{c}\n",
    "      \\text { independent } \\\\\n",
    "      \\text { zero mean }\n",
    "    \\end{array}\n",
    "  \\!\\right\\}\n",
    "\\left.\\!\n",
    "    \\begin{array}{c}\n",
    "      \\implies \\\\\n",
    "      \\not\\impliedby\n",
    "    \\end{array}\n",
    "  \\!\\right.\n",
    "\\left\\{\\!\n",
    "    \\begin{array}{c}\n",
    "      \\text { uncorrelated } \\\\\n",
    "      \\text { zero mean}\n",
    "    \\end{array}\n",
    "  \\!\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b19e38-ea7c-41f4-896c-1235156c1fe2",
   "metadata": {},
   "source": [
    "- joint distribution of:\n",
    "$$\\text {  the independent Gaussian } \\mathbf{r.v.} \\text{â€™s } {\\color{Cyan}z}_i. $$\n",
    "$$ \n",
    "f_{\\color{Cyan}z}(Z) \n",
    "= \n",
    "\\prod_{i = 1}^m f_{\\color{Cyan}z}(z_i) \n",
    "= \n",
    "\\prod_{i = 1}^m exp\\Big( - \\frac{z_i^2}{2\\lambda_i}\\Big) \n",
    "= \n",
    "(2\\pi)^{- \\frac{m}{2}} det(\\Lambda)^{- \\frac{1}{2}}exp\\Big( - \\frac{1}{2}Z^T \\Lambda^{-1} Z \\Big) \n",
    "$$\n",
    "<img src=images/joint_distribution.png width=\"20%\" height=\"20%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa10a5-6804-4623-ae8f-77f5c462722b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Since the Jacobian:\n",
    "$$\n",
    "\\begin{gather} \n",
    "(det V^T) \\text{ of the linear transformation between } {\\color{YellowGreen}X} \\text { and } {\\color{Cyan}Z} \\text{ equals one, }\n",
    "\\\\\n",
    "\\text{ we get for the joint distribution of the } x_i  \\quad {\\color{Cyan}Z} = V^T ( {\\color{YellowGreen}X} - m_X )\n",
    "\\\\\n",
    "\\\\\n",
    "f_{\\color{YellowGreen}X}(X) = f_{\\color{YellowGreen}Z} \\Big( V^T({\\color{YellowGreen}X} - m_X) \\Big) \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad\n",
    "\\\\\n",
    "= \n",
    "(2\\pi)^{- \\frac{m}{2}} \\big( det(V^T C_{XX} V) \\big)^{- \\frac{1}{2}}  exp \\Big( - \\frac{1}{2}[V^T (X - m_x)]^T  \\; \\Lambda^{-1} \\; [V^T (X - m_x)] \\; \\Big) \n",
    "\\end{gather}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25208-5fde-4c74-8cb3-b8a61ab257e2",
   "metadata": {},
   "source": [
    "### how to derivate a multivariate Gaussian Distribution from an univariate Gaussian distribution and two postulates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb60486-2f94-49e5-86f9-d9041181813b",
   "metadata": {},
   "source": [
    "Deriving a multivariate Gaussian distribution from a univariate Gaussian distribution and a set of postulates is a classic exercise in statistics and probability theory. This derivation typically involves using certain assumptions or postulates to extend the univariate Gaussian to a multivariate case. Here's a basic outline of how this can be done:\n",
    "\n",
    "**Assumptions (Postulates):**\n",
    "\n",
    "1. **Independence**: We assume that the variables involved in the multivariate Gaussian distribution are independent. In other words, the probability of observing one variable is not affected by the values of the other variables.\n",
    "\n",
    "2. **Univariate Gaussian Distribution**: We start with the assumption that each individual variable in the multivariate distribution follows a univariate Gaussian (also known as a normal) distribution with its own mean and variance.\n",
    "\n",
    "**Derivation Steps:**\n",
    "\n",
    "1. **Univariate Gaussian Distribution**:\n",
    "   Start with the univariate Gaussian probability density function (PDF) for a single random variable, X:\n",
    "\n",
    "   $$ f(X; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(X - \\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "   $$ \\text { Here,  } \\mu \\text{ represents the mean and } \\sigma^2 \\text { represents the variance. } $$\n",
    "\n",
    "2. **Multivariate Gaussian Distribution**:\n",
    "   Now, we extend this to a multivariate case with n variables, X1, X2, ..., Xn, assuming independence:\n",
    "\n",
    "   $$ f(\\mathbf{X}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^n |\\boldsymbol{\\Sigma}|}}e^{-\\frac{1}{2} (\\mathbf{X} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{X} - \\boldsymbol{\\mu})} $$\n",
    "\n",
    "In this equation:\n",
    "    $$ \n",
    "    \\begin{gather}\n",
    "       - \\mathbf{X} \\text { represents a vector of the n random variables. } \\\\\n",
    "       - \\boldsymbol{\\mu} \\text { represents a vector of means for each variable. } \\\\\n",
    "       - \\boldsymbol{\\Sigma} \\text { represents the covariance matrix of the variables. } \\\\\n",
    "       - |\\boldsymbol{\\Sigma}| \\text { is the determinant of the covariance matrix. }\n",
    "    \\end{gather}\n",
    "    $$\n",
    "\n",
    "3. **Covariance Matrix**:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\qquad\n",
    "\\text{ The covariance matrix } \\boldsymbol{\\Sigma} \\text { captures the relationships between variables and is crucial in defining the multivariate Gaussian. } \\\\\n",
    "\\text { It is calculated based on the variances and covariances of the individual variables. }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\qquad\n",
    "\\text{ For example, if variables X1 and X2 have variances } \\sigma_1^2 \\text { and } \\sigma_2^2 \\text {, respectively, and a covariance } \\sigma_{12} \\\\\n",
    "\\text { between them, then the covariance matrix }  \\boldsymbol{\\Sigma} \\text { would be: }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n",
    "      \\sigma_1^2 & \\sigma_{12} \\\\\n",
    "      \\sigma_{12} & \\sigma_2^2\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "This derivation extends the concept of a univariate Gaussian distribution to multiple variables while maintaining the independence assumption and using the covariance matrix to account for the relationships between variables. It's important to note that the multivariate Gaussian distribution plays a crucial role in many areas of statistics and data analysis, particularly when dealing with correlated or multivariate data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
