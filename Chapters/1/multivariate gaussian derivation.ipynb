{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc60221a-a09b-4c92-a08c-66c58fcda3c8",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4920f-c733-460c-a235-92bfa3252494",
   "metadata": {},
   "source": [
    " - [ ]  _goal:_ derive multivariate Gaussian distribution from univariate Gaussian distribution and two postulates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d4e0f-187e-4579-88cd-12ffcb480602",
   "metadata": {},
   "source": [
    "$$ \n",
    "{\\color{YellowGreen}X} = [x_1 \\dots x_m]^T \\text{ real random vector with specified mean and covariance matrix:} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3998f4a-86b5-4339-ae84-4578a3c9a7d0",
   "metadata": {},
   "source": [
    "- with mean:\n",
    "\n",
    "$$\n",
    "m_X = E{\\color{YellowGreen}X} = [m_{x_1} \\dots m_{x_m}]^T  \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f8d1e-0f0b-4efe-b8e5-0a45b9c42eb2",
   "metadata": {},
   "source": [
    "- and covariance matrix: (centered)\n",
    "$$\n",
    "\\begin{gather}\n",
    "C_{XX} = E({\\color{YellowGreen}X}-m_X)({\\color{YellowGreen}X}-m_X)^T = \\Big[ E({\\color{YellowGreen}X}-m_X)({\\color{YellowGreen}X}-m_X)^T \\Big]_{i,j=1}^m\n",
    "\\\\\n",
    "=\n",
    "E({\\color{YellowGreen}XX}^T-{\\color{YellowGreen}X}m_X^T-m_X{\\color{YellowGreen}X}^T+m_Xm_X^T) \\qquad \\qquad \\quad\n",
    "\\\\\n",
    "= \n",
    "(E{\\color{YellowGreen}X}{\\color{YellowGreen}X}^T) - (E{\\color{YellowGreen}X})m_X^T-m_X(E{\\color{YellowGreen}X})^T + m_Xm_X^T \\qquad\n",
    "\\\\\n",
    "\\qquad = R_{XX}-m_Xm_X^T-\\not{m_Xm_X^T}+\\not{m_Xm_X^T} = R_{XX}-m_Xm_X^T\n",
    "\\end{gather}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fce052-2de3-48a7-ae03-85170cd07d25",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{gather}\n",
    "R_{XX} = E{\\color{YellowGreen}X}{\\color{YellowGreen}X}^T = \\text{ correlation matrix } \\mathit{(not \\space centered)} \\qquad \\qquad \\quad\n",
    "\\\\\n",
    "\\text{ linearity of expectation } \\mathit{E} \\text { exploited } \\mathit{(linear \\space operations \\space commute)}\n",
    "\\end{gather}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb6074-46fa-499e-bb6e-10e29223dd00",
   "metadata": {},
   "source": [
    "- by definition of expectation\n",
    "$$ \n",
    "C_{XX} = E({\\color{YellowGreen}X}-m_X)({\\color{YellowGreen}X}-m_X)^T = \\int \\mathrm{d}x_1 \\dots \\int \\mathrm{d}x_m f_{\\color{YellowGreen}X}(X) ({\\color{YellowGreen}X} - m_x)({\\color{YellowGreen}X} - m_x)^T\n",
    "\\implies C_{XX} = C_{XX}^T \\text { symmetric }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbf6b9-879a-4252-93aa-cd28d006657b",
   "metadata": {},
   "source": [
    "- _Postulate_ that a linear transformation of jointly Gaussian random variables produces again jointly Gaussian random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215b258-3e57-415c-9924-2afb42fe84f5",
   "metadata": {},
   "source": [
    "- Consider now a linear transformation of variables:\n",
    "$$ \n",
    "\\begin{gather}\n",
    "{\\color{Cyan}Z} = V^T({\\color{YellowGreen}X} - m_X)\n",
    "\\\\\n",
    "\\text { Components of } {\\color{Cyan}Z} \\text{ jointly Gaussian }\n",
    "\\\\\n",
    "{\\color{YellowGreen}X} \\text{ is Gaussian } \\iff {\\color{Cyan}Z} \\text { is Gaussian }\n",
    "\\end{gather}\n",
    "$$\n",
    "$$ \n",
    "\\begin{gather}\n",
    "\\text{ Then we find the first two moments: } \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad\n",
    "\\\\\n",
    "m_Z = E \\; V^T({\\color{YellowGreen}X} - m_X ) = V^T(E{\\color{YellowGreen}X} - m_X ) = V^T(m_X - m_X) = 0 \\qquad \\qquad \\qquad\n",
    "\\\\\n",
    "C_{ZZ} = E({\\color{Cyan}Z} - m_Z)({\\color{Cyan}Z} - m_Z)^T = E{\\color{Cyan}Z}{\\color{Cyan}Z}^T = EV^T ({\\color{YellowGreen}X} - m_X)({\\color{YellowGreen}X} - m_X)^TV \\qquad\n",
    "\\\\\n",
    "= \n",
    "V^T \\big( E({\\color{YellowGreen}X} - m_X)({\\color{YellowGreen}X} - m_X)^T \\big) V = V^TC_{XX}V = V^TV \\Lambda V^TV = \\Lambda\n",
    "\\\\\n",
    "\\qquad \\qquad  \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\widehat { \\mathit{eigendecomposition} }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\text{ Or hence } \\mathit{E}_{ {\\color{Cyan}z}_i{\\color{Cyan}z}_j } = \\lambda_i \\sigma_{ij} \\text{ : the } {\\color{Cyan}z}_i \\text{ are zero mean and uncorrelated } \\qquad \\qquad\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d798098-513f-4a90-adb5-6d1e9f566995",
   "metadata": {},
   "source": [
    "- At this point, only are specified : the first two moments of the ${\\color{Cyan}z}_i$. Now, specify the rest of their distribution by stating that the ${\\color{Cyan}z}_i$ are jointly Gaussian. We furthermore postulate that zero mean uncorrelated Gaussian random variables are independent.\n",
    "\n",
    "Note that in general\n",
    "$\n",
    "\\left.\\!\n",
    "    \\begin{array}{c}\n",
    "      \\text { independent } \\\\\n",
    "      \\text { zero mean }\n",
    "    \\end{array}\n",
    "  \\!\\right\\}\n",
    "\\left.\\!\n",
    "    \\begin{array}{c}\n",
    "      \\implies \\\\\n",
    "      \\not\\impliedby\n",
    "    \\end{array}\n",
    "  \\!\\right.\n",
    "\\left\\{\\!\n",
    "    \\begin{array}{c}\n",
    "      \\text { uncorrelated } \\\\\n",
    "      \\text { zero mean}\n",
    "    \\end{array}\n",
    "  \\!\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b19e38-ea7c-41f4-896c-1235156c1fe2",
   "metadata": {},
   "source": [
    "- joint distribution of:\n",
    "$\\text {  the independent Gaussian } \\mathbf{r.v.} \\text{â€™s } {\\color{Cyan}z}_i. $\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "f_{\\color{Cyan}z}(Z) \n",
    "= \n",
    "\\prod_{i = 1}^m f_{\\color{Cyan}z}(z_i) \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\;\n",
    "\\\\\n",
    "= \n",
    "\\prod_{i = 1}^m exp\\Big( - \\frac{z_i^2}{2\\lambda_i}\\Big) \\qquad \\qquad \\qquad \\qquad  \\quad\n",
    "\\\\\n",
    "= \n",
    "(2\\pi)^{- \\frac{m}{2}} det(\\Lambda)^{- \\frac{1}{2}}exp\\Big( - \\frac{1}{2}Z^T \\Lambda^{-1} Z \\Big)\n",
    "\\\\\n",
    "\\end{split}\n",
    "$\n",
    "<img src=images/joint_distribution_derivation.png width='15%' height='15%' />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa10a5-6804-4623-ae8f-77f5c462722b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Since the Jacobian: $(det V^T)$ of the linear transformation between ${\\color{YellowGreen}X}$ and ${\\color{Cyan}Z}$ equals one, we get for the joint distribution of the $x_i$  ${\\color{Cyan}Z} = V^T ( {\\color{YellowGreen}X} - m_X )$\n",
    "$$\n",
    "\\begin{gather}\n",
    "f_{\\color{YellowGreen}X}(X) = f_{\\color{YellowGreen}Z} \\Big( V^T({\\color{YellowGreen}X} - m_X) \\Big) \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad\n",
    "\\\\\n",
    "= \n",
    "(2\\pi)^{- \\frac{m}{2}} \\big( det(V^T C_{XX} V) \\big)^{- \\frac{1}{2}}  exp \\Big( - \\frac{1}{2}[V^T (X - m_x)]^T  \\; \\Lambda^{-1} \\; [V^T (X - m_x)] \\; \\Big) \n",
    "\\end{gather}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25208-5fde-4c74-8cb3-b8a61ab257e2",
   "metadata": {},
   "source": [
    "### how to derivate a multivariate Gaussian Distribution from an univariate Gaussian distribution and two postulates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb60486-2f94-49e5-86f9-d9041181813b",
   "metadata": {},
   "source": [
    "Deriving a multivariate Gaussian distribution from a univariate Gaussian distribution and a set of postulates is a classic exercise in statistics and probability theory. This derivation typically involves using certain assumptions or postulates to extend the univariate Gaussian to a multivariate case. Here's a basic outline of how this can be done:\n",
    "\n",
    "**Assumptions (Postulates):**\n",
    "\n",
    "1. **Independence**: We assume that the variables involved in the multivariate Gaussian distribution are independent. In other words, the probability of observing one variable is not affected by the values of the other variables.\n",
    "\n",
    "2. **Univariate Gaussian Distribution**: We start with the assumption that each individual variable in the multivariate distribution follows a univariate Gaussian (also known as a normal) distribution with its own mean and variance.\n",
    "\n",
    "**Derivation Steps:**\n",
    "\n",
    "1. **Univariate Gaussian Distribution**:\n",
    "   Start with the univariate Gaussian probability density function (PDF) for a single random variable, X:\n",
    "\n",
    "   $$ f(X; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(X - \\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "   $$ \\text { Here,  } \\mu \\text{ represents the mean and } \\sigma^2 \\text { represents the variance. } $$\n",
    "\n",
    "2. **Multivariate Gaussian Distribution**:\n",
    "   Now, we extend this to a multivariate case with n variables, X1, X2, ..., Xn, assuming independence:\n",
    "\n",
    "   $$ f(\\mathbf{X}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^n |\\boldsymbol{\\Sigma}|}}e^{-\\frac{1}{2} (\\mathbf{X} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{X} - \\boldsymbol{\\mu})} $$\n",
    "\n",
    "In this equation:\n",
    "    $$ \n",
    "    \\begin{gather}\n",
    "       - \\mathbf{X} \\text { represents a vector of the n random variables. } \\\\ \n",
    "       - \\boldsymbol{\\mu} \\text { represents a vector of means for each variable. } \\\\ \n",
    "       - \\boldsymbol{\\Sigma} \\text { represents the covariance matrix of the variables. } \\\\ \n",
    "       - |\\boldsymbol{\\Sigma}| \\text { is the determinant of the covariance matrix. }\n",
    "    \\end{gather}\n",
    "    $$\n",
    "\n",
    "3. **Covariance Matrix**:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\qquad\n",
    "\\text{ The covariance matrix } \\boldsymbol{\\Sigma} \\text { captures the relationships between variables and is crucial in defining the multivariate Gaussian. } \\\\\n",
    "\\text { It is calculated based on the variances and covariances of the individual variables. }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\qquad\n",
    "\\text{ For example, if variables X1 and X2 have variances } \\sigma_1^2 \\text { and } \\sigma_2^2 \\text {, respectively, and a covariance } \\sigma_{12} \\\\\n",
    "\\text { between them, then the covariance matrix }  \\boldsymbol{\\Sigma} \\text { would be: }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n",
    "      \\sigma_1^2 & \\sigma_{12} \\\\\n",
    "      \\sigma_{12} & \\sigma_2^2\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "This derivation extends the concept of a univariate Gaussian distribution to multiple variables while maintaining the independence assumption and using the covariance matrix to account for the relationships between variables. It's important to note that the multivariate Gaussian distribution plays a crucial role in many areas of statistics and data analysis, particularly when dealing with correlated or multivariate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbe272-8b24-48e5-a9e8-e4a9ee6f9725",
   "metadata": {},
   "source": [
    "## why is eigenfunction needed in derivating multivariate gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27c87c-f705-4f87-8053-88b52a01dcc0",
   "metadata": {},
   "source": [
    "In the context of multivariate Gaussian distributions, eigenfunctions play a role when we are dealing with the covariance matrix and, in particular, when expressing the density function of the distribution. The eigendecomposition of the covariance matrix helps in diagonalizing it, which simplifies certain computations and allows us to express the multivariate Gaussian distribution in a more convenient form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0955afa-bc25-42ac-b425-f6c0af29adb3",
   "metadata": {},
   "source": [
    "\n",
    "$$\\text { Let's consider the multivariate Gaussian density function for a vector } x \\text { with mean } \\mu \\text { and covariance matrix } \\Sigma$$\n",
    "\n",
    "$$f(x; \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^k \\text{det}(\\Sigma)}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right)$$\n",
    "\n",
    "\n",
    "$$\\text{ Here, } k \\text{ is the dimensionality of the multivariate distribution, and } \\text{det}(\\Sigma) \\text { is the determinant of the covariance matrix. }$$\n",
    "\n",
    "$$\\text{ Now, if the covariance matrix } \\Sigma \\text{ is symmetric, it can be diagonalized using its eigendecomposition: }$$\n",
    "\n",
    "$$ \\Sigma = V \\Lambda V^T $$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "- V \\text{ is a matrix whose columns are the eigenvectors of } \\Sigma \\qquad \\qquad\n",
    "\\\\\n",
    "- \\Lambda \\text{ is a diagonal matrix whose diagonal elements are the eigenvalues of } \\Sigma\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Using this eigendecomposition, we can express the inverse of the covariance matrix as:\n",
    "\n",
    "$$ \\Sigma^{-1} = V \\Lambda^{-1} V^T $$\n",
    "\n",
    "Substituting this into the multivariate Gaussian density function, we get:\n",
    "\n",
    "$$ f(x; \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^k \\text{det}(\\Sigma)}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T V \\Lambda^{-1} V^T (x - \\mu)\\right) $$\n",
    "\n",
    "\n",
    "Now, if we define a new variable: $$y = V^T (x - \\mu)$$\n",
    "\n",
    "the density function becomes:\n",
    "\n",
    "$$ f(y; \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^k \\text{det}(\\Lambda)}} \\exp\\left(-\\frac{1}{2}y^T \\Lambda^{-1} y\\right) $$\n",
    "\n",
    "$$\\text { Here, } \\Lambda \\text{ is a diagonal matrix, and } \\text{det}(\\Lambda) \\text{ is simply the product of its diagonal elements.}$$\n",
    "\n",
    "The multivariate Gaussian distribution is now decoupled along the different dimensions, and we have k independent univariate Gaussian distributions.\n",
    "\n",
    "The eigenfunctions (eigenvectors) play a crucial role in this diagonalization process, simplifying the expression of the distribution and making it more amenable to analysis and computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96873dfe-458e-4a0d-8a57-e50995ab135a",
   "metadata": {},
   "source": [
    "## how many derivation order to get to the global maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb0c8b-f329-4666-83c2-81cce13834c3",
   "metadata": {},
   "source": [
    "The number of derivative orders required to reach the global maximum of a function depends on the characteristics of the function. Specifically, it depends on the degree and complexity of the function, as well as the presence of local maxima or minima.\n",
    "\n",
    "In general, if a function is smooth and has a single global maximum (and no other local maxima), you would typically need to take the first derivative and set it equal to zero to find the critical points. These critical points could correspond to either maxima, minima, or saddle points. You would then use the second derivative test to determine whether each critical point is a maximum, minimum, or inconclusive.\n",
    "\n",
    "Here's a general guide:\n",
    "\n",
    "1. **First Derivative (Gradient):**\n",
    "   - Find the critical points by setting the first derivative equal to zero: \\(f'(x) = 0\\).\n",
    "\n",
    "2. **Second Derivative (Hessian):**\n",
    "   - Use the second derivative test to analyze the critical points. If \\(f''(x) > 0\\), the critical point is a local minimum. If \\(f''(x) < 0\\), it's a local maximum. If \\(f''(x) = 0\\), the test is inconclusive.\n",
    "\n",
    "3. **Higher Derivatives:**\n",
    "   - If the second derivative test is inconclusive (e.g., if \\(f''(x) = 0\\)), you might need to examine higher-order derivatives or use additional techniques to determine the nature of the critical points.\n",
    "\n",
    "It's important to note that not all functions have a global maximum. Functions with multiple local maxima or complex shapes may require more sophisticated optimization methods, and the analytical approach described above may not be sufficient. Numerical optimization methods, such as gradient descent, are often used in practice for finding the maximum or minimum of a function.\n",
    "\n",
    "In summary, the number of derivative orders needed to find the global maximum depends on the nature and complexity of the function. For simple, smooth functions with a single global maximum, the first and second derivatives are often sufficient. For more complex functions, numerical optimization methods may be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1fb8a-4e27-4496-b0f8-18993c63cdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
