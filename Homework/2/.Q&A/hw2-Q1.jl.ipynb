{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6d685c-8770-41cc-81f8-aba40c7f810c",
   "metadata": {},
   "source": [
    "# Bayesian Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73282ae6-9e8c-489f-a840-24ace7126168",
   "metadata": {},
   "source": [
    "#### ${\\color{Cyan} 1. }$ On the Beneficial Bias of MMSE Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdf86b-e2e2-48ee-bfbc-119bb661d42a",
   "metadata": {},
   "source": [
    "Consider the Bayesian linear model $Y = H\\theta+V$ with $θ ∼ N(0,C_{θθ})$ and $V ∼ N(\\theta,C_{VV} )$ independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36a778-baf6-4974-8be4-59de7115780c",
   "metadata": {},
   "source": [
    "The Bayesian linear model you've provided is described by the equation:\n",
    "\n",
    "$ Y = H \\theta + V $\n",
    "\n",
    "where:\n",
    "- $ Y $ is the observed data vector.\n",
    "- $ H $ is the design matrix.\n",
    "- $ \\theta $ is the parameter vector with a prior distribution $ \\theta \\sim \\mathcal{N}(0, C_{\\theta \\theta}) $.\n",
    "- $ V $ is the noise vector with $ V \\sim \\mathcal{N}(0, C_{V V}) $.\n",
    "\n",
    "This model assumes that the parameter vector $ \\theta $ follows a normal distribution with mean zero and covariance matrix $ C_{\\theta \\theta} $, and the noise vector $ V $ follows a normal distribution with mean zero and covariance matrix $ C_{V V} $. The parameters $ \\theta $ and $ V $ are assumed to be independent.\n",
    "\n",
    "To perform Bayesian inference in this context, you typically want to estimate the posterior distribution of $ \\theta $ given the observed data $ Y $. The posterior distribution is given by Bayes' theorem:\n",
    "\n",
    "$ P(\\theta | Y) \\propto P(Y | \\theta) P(\\theta) $\n",
    "\n",
    "where:\n",
    "- $ P(\\theta | Y) $ is the posterior distribution.\n",
    "- $ P(Y | \\theta) $ is the likelihood, given by the likelihood of the observed data given the parameters.\n",
    "- $ P(\\theta) $ is the prior distribution of the parameters.\n",
    "\n",
    "The specific details of Bayesian inference, including the choice of prior distributions, the form of the likelihood, and the computation of the posterior, depend on the specific problem you are trying to solve.\n",
    "\n",
    "If you have specific data and would like to perform Bayesian inference in Julia or have questions about a particular aspect of the model, please provide more details so that I can assist you more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236db3b-5979-4982-8118-f750184b9446",
   "metadata": {},
   "source": [
    "- [ ] we consider here mθ = 0 for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9272f-416f-4eee-b788-007cdd81c613",
   "metadata": {},
   "source": [
    "If you're considering a prior distribution $ \\theta \\sim \\mathcal{N}(0, C_{\\theta \\theta}) $ with a mean $ m_{\\theta} = 0 $, it simplifies the model. The prior distribution for $ \\theta $ is then fully characterized by the covariance matrix $ C_{\\theta \\theta} $. With this simplification, the Bayesian linear model becomes:\n",
    "\n",
    "$ Y = H \\theta + V $\n",
    "\n",
    "where:\n",
    "- $ Y $ is the observed data vector.\n",
    "- $ H $ is the design matrix.\n",
    "- $ \\theta $ is the parameter vector with $ \\theta \\sim \\mathcal{N}(0, C_{\\theta \\theta}) $.\n",
    "- $ V $ is the noise vector with $ V \\sim \\mathcal{N}(0, C_{V V}) $.\n",
    "\n",
    "To perform Bayesian inference in this case, you would update the prior to $ P(\\theta) = \\mathcal{N}(0, C_{\\theta \\theta}) $, and the posterior distribution is given by:\n",
    "\n",
    "$ P(\\theta | Y) \\propto P(Y | \\theta) P(\\theta) $\n",
    "\n",
    "Where:\n",
    "- $ P(\\theta | Y) $ is the posterior distribution.\n",
    "- $ P(Y | \\theta) $ is the likelihood.\n",
    "- $ P(\\theta) $ is the prior distribution.\n",
    "\n",
    "Given the mean $ m_{\\theta} = 0 $, the entire information about the prior distribution is contained in the covariance matrix $ C_{\\theta \\theta} $. If you have data $ Y $ and want to perform Bayesian inference, you would need to specify the likelihood $ P(Y | \\theta) $ based on your specific problem.\n",
    "\n",
    "If you have specific data and would like to proceed with Bayesian inference in Julia or have further questions, feel free to provide more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668bffd-dbe1-42ec-9557-1f6ca3d7c8f7",
   "metadata": {},
   "source": [
    "${\\color{Cyan}(a)}$ The LMMSE estimator is $\\hat{\\theta}_{LMMSE} = C_{\\theta Y}C_{YY}^{-1}Y =(C_{\\theta\\theta}^{−1}+H^T C_{VV}^{−1}H)^{−1} H^T C_{VV}^{−1}Y. $\n",
    "What are the unconstrained (non-linear) MMSE and the MAP estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180fcf71-f6e7-4cd5-bfdf-3ce1322b1439",
   "metadata": {},
   "source": [
    "The Linear Minimum Mean Squared Error (LMMSE) estimator is given by:\n",
    "\n",
    "$ \\hat{\\theta}_{\\text{LMMSE}} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y $\n",
    "\n",
    "Let's revisit the Unconstrained Minimum Mean Squared Error (MMSE) and the Maximum A Posteriori (MAP) estimators.\n",
    "\n",
    "1. **Unconstrained (Non-linear) MMSE Estimator:**\n",
    "\n",
    "The unconstrained MMSE estimator is obtained by minimizing the mean squared error without any constraints on the parameter. In the Bayesian framework, the unconstrained MMSE estimator is also known as the posterior mean, and it is given by the mean of the posterior distribution. In the case where $ \\theta $ follows a normal distribution, the posterior mean is equal to the posterior distribution's mean:\n",
    "\n",
    "$ \\hat{\\theta}_{\\text{MMSE}} = (C_{\\theta \\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y $\n",
    "\n",
    "This is the same expression as the LMMSE estimator in this case.\n",
    "\n",
    "2. **Maximum A Posteriori (MAP) Estimator:**\n",
    "\n",
    "The MAP estimator seeks the most probable value of the parameter given the observed data and the prior information. It is obtained by maximizing the posterior distribution:\n",
    "\n",
    "$ \\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} P(\\theta | Y) $\n",
    "\n",
    "In the case of a Gaussian prior, this is equivalent to minimizing the negative log posterior, and the MAP estimator is obtained as:\n",
    "\n",
    "$ \\hat{\\theta}_{\\text{MAP}} = \\arg \\min_{\\theta} \\left[ \\frac{1}{2} (Y - H \\theta)^T C_{VV}^{-1} (Y - H \\theta) + \\frac{1}{2} \\theta^T C_{\\theta \\theta}^{-1} \\theta \\right] $\n",
    "\n",
    "This minimization problem can be solved using optimization techniques. The solution is a compromise between fitting the observed data and staying close to the prior distribution.\n",
    "\n",
    "In summary, both the unconstrained MMSE and the MAP estimators in this case turn out to be the same as the LMMSE estimator due to the Gaussian assumption for the prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab6ab4-2c85-4ed4-8397-2e6cef78aa09",
   "metadata": {},
   "source": [
    "${\\color{Cyan}(b)}$ What is the error covariance matrix ?\n",
    "$R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} = E_\\theta E_{Y|\\theta} \\tilde{\\theta}_{LMMSE} \\tilde{\\theta}_{LMMSE}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e53ee-9dd3-47a6-b7f2-6de6790039fd",
   "metadata": {},
   "source": [
    "The error covariance matrix, denoted as $R_{\\tilde{\\theta}\\tilde{\\theta}}^{\\text{LMMSE}}$, represents the covariance matrix of the estimation error $\\tilde{\\theta}_{\\text{LMMSE}}$, where:\n",
    "\n",
    "$ \\tilde{\\theta}_{\\text{LMMSE}} = \\theta - \\hat{\\theta}_{\\text{LMMSE}} $\n",
    "\n",
    "Here, $\\theta$ is the true parameter vector, and $\\hat{\\theta}_{\\text{LMMSE}}$ is the LMMSE estimator.\n",
    "\n",
    "To calculate $R_{\\tilde{\\theta}\\tilde{\\theta}}^{\\text{LMMSE}}$, we need the expectations under $E_\\theta$ and $E_{Y|\\theta}$ of the outer product of the estimation error. It is important to note that $E_\\theta$ is the expectation with respect to the true parameter $\\theta$, and $E_{Y|\\theta}$ is the conditional expectation given the observed data $Y$ and the true parameter $\\theta$.\n",
    "\n",
    "The error covariance matrix is given by:\n",
    "\n",
    "$ R_{\\tilde{\\theta}\\tilde{\\theta}}^{\\text{LMMSE}} = E_\\theta E_{Y|\\theta} (\\tilde{\\theta}_{\\text{LMMSE}} \\tilde{\\theta}_{\\text{LMMSE}}^T) $\n",
    "\n",
    "Given that $\\tilde{\\theta}_{\\text{LMMSE}} = \\theta - \\hat{\\theta}_{\\text{LMMSE}}$, we can substitute this expression:\n",
    "\n",
    "$ R_{\\tilde{\\theta}\\tilde{\\theta}}^{\\text{LMMSE}} = E_\\theta E_{Y|\\theta} ((\\theta - \\hat{\\theta}_{\\text{LMMSE}}) (\\theta - \\hat{\\theta}_{\\text{LMMSE}})^T) $\n",
    "\n",
    "Expanding this expression involves taking expectations over both $\\theta$ and $Y|\\theta$. The exact form of $R_{\\tilde{\\theta}\\tilde{\\theta}}^{\\text{LMMSE}}$ will depend on the specific details of the prior distribution, the likelihood function, and the observation model.\n",
    "\n",
    "It's important to note that the calculation of this error covariance matrix involves complex integrations or summations and may require specific assumptions about the statistical properties of the prior and likelihood. In practical situations, numerical methods or simulations may be used for these calculations. If you have specific details about the prior and likelihood, I can provide more tailored guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36467878-2f0d-4448-b3dc-e7269d37d271",
   "metadata": {},
   "source": [
    "${\\color{Cyan}(c)}$ The conditional bias of an estimator \n",
    "$\\hat{\\theta}$ is $b_{\\hat{\\theta}}(\\theta) = E_{y|\\theta} \\hat{\\theta} (Y) - \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72dbec3-31b4-4561-a535-ea18580fc45b",
   "metadata": {},
   "source": [
    "The conditional bias of an estimator $\\hat{\\theta}$, denoted as $b_{\\hat{\\theta}}(\\theta)$, is defined as the expected difference between the estimator and the true parameter value $\\theta$, given the true parameter value $\\theta$. It is calculated as:\n",
    "\n",
    "$ b_{\\hat{\\theta}}(\\theta) = E_{Y|\\theta} [\\hat{\\theta}(Y)] - \\theta $\n",
    "\n",
    "Here,\n",
    "- $E_{Y|\\theta} [\\hat{\\theta}(Y)]$ is the conditional expectation of the estimator given the true parameter value $\\theta$.\n",
    "- $\\theta$ is the true parameter value.\n",
    "\n",
    "The conditional bias measures the systematic error of the estimator when the true parameter value is known. If the conditional bias is zero for all possible values of $\\theta$, the estimator is said to be unbiased.\n",
    "\n",
    "It's important to note that the expectation $E_{Y|\\theta} [\\hat{\\theta}(Y)]$ is taken over the distribution of the estimator $\\hat{\\theta}$ given the observed data $Y$ and the true parameter value $\\theta$.\n",
    "\n",
    "In the context of the Linear Minimum Mean Squared Error (LMMSE) estimator, the conditional bias $b_{\\hat{\\theta}_{\\text{LMMSE}}}(\\theta)$ is calculated as:\n",
    "\n",
    "$ b_{\\hat{\\theta}_{\\text{LMMSE}}}(\\theta) = E_{Y|\\theta} [\\hat{\\theta}_{\\text{LMMSE}}(Y)] - \\theta $\n",
    "\n",
    "where $\\hat{\\theta}_{\\text{LMMSE}}(Y)$ is the LMMSE estimator given the observed data $Y$, and the expectation is taken with respect to the conditional distribution of $Y$ given the true parameter value $\\theta$.\n",
    "\n",
    "The specific calculation of $b_{\\hat{\\theta}_{\\text{LMMSE}}}(\\theta)$ involves integrating or summing over the conditional distribution of $Y$ given $\\theta$ and depends on the specific details of the prior, likelihood, and observation model. If you have more specific information about the distribution of $Y|\\theta$ and the prior, I can provide more detailed guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdea91-2e25-4308-b411-50bab8796385",
   "metadata": {},
   "source": [
    "${\\color{Cyan} * }$ The BLUE estimator is the LMMSE estimator under the constraint of conditional\n",
    "unbiasedness. So $b_{BLUE}(\\theta) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1252b498-7e4b-42ad-b743-6f0c492f9426",
   "metadata": {},
   "source": [
    "Yes, that's correct. The Best Linear Unbiased Estimator (BLUE) is essentially the Linear Minimum Mean Squared Error (LMMSE) estimator under the constraint of conditional unbiasedness. This constraint ensures that, for every possible true parameter value $\\theta$, the conditional expectation of the estimator given the observed data is equal to $\\theta$.\n",
    "\n",
    "Mathematically, for the BLUE estimator $\\hat{\\theta}_{\\text{BLUE}}$, the conditional bias is zero for all $\\theta$:\n",
    "\n",
    "$ b_{\\hat{\\theta}_{\\text{BLUE}}}(\\theta) = E_{Y|\\theta} [\\hat{\\theta}_{\\text{BLUE}}(Y)] - \\theta = 0 $\n",
    "\n",
    "This condition of conditional unbiasedness makes the BLUE estimator particularly appealing, as it combines linearity (L) and unbiasedness (U) under the minimum mean squared error criterion.\n",
    "\n",
    "In the context of the LMMSE estimator, the BLUE estimator is obtained by setting the constraint:\n",
    "\n",
    "$ E_{Y|\\theta} [\\hat{\\theta}_{\\text{LMMSE}}(Y)] - \\theta = 0 $\n",
    "\n",
    "This constraint is satisfied by the LMMSE estimator, and hence, the LMMSE estimator is also the BLUE estimator in this case.\n",
    "\n",
    "In summary, for the BLUE estimator, the conditional bias is zero, ensuring conditional unbiasedness. The LMMSE estimator, under this condition, becomes the BLUE estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877f533-70df-4db5-8b11-2978b9da2c39",
   "metadata": {},
   "source": [
    "${\\color{Cyan} * }$ What is $\\hat{\\theta}_{BLUE}$ in terms of the quantities appearing in the Bayesian linear model considered here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845c65c-a767-4a0b-b8c5-8c63ac2bd8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
