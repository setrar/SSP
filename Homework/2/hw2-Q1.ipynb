{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2cf2736-0a7c-449c-975a-3c894fd5487b",
   "metadata": {},
   "source": [
    "# ${\\color{Blue} \\text{ Bayesian Parameter Estimation}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea806e-6b6b-4915-a035-4d59ad3bda89",
   "metadata": {},
   "source": [
    "### ${\\color{Purple}1.} {\\color{Blue} \\text{On the Beneficial Bias of MMSE Estimation}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40d8c7-dbcb-42fa-8df5-9eb54bae99de",
   "metadata": {},
   "source": [
    "${\\color{Blue} \\text {Consider the Bayesian linear model} Y = H \\theta+V \\text{ with } \\theta \\sim N(0,C_{\\theta\\theta}) \\text{ and } V \\sim N(0,C_{VV} ) \\text{ independent (we consider here } m_{\\theta} = 0 \\text{ for simplicity) } .}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618c649-90dc-44b4-8e6a-a34d39768977",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(a)}$ The LMMSE estimator is $\\hat{\\theta}_{LMMSE} = C_{\\theta Y} C_{YY}^{−1}Y =(C_{\\theta\\theta}^{−1}+ H^T C_{VV}^{−1} H)^{−1} H^T C_{VV}^{−1} Y $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474431d-6b8f-49ae-873a-c1408da605aa",
   "metadata": {},
   "source": [
    "${\\color{Green}-}$  What are the unconstrained (non-linear) MMSE and the MAP estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dee58f-6058-4dcf-bcf5-012affc462ed",
   "metadata": {},
   "source": [
    "${\\color{Salmon}Response:}$\n",
    "\n",
    "The Linear Minimum Mean Squared Error (LMMSE) estimator is given by:\n",
    "\n",
    "$ \\hat{\\theta}_{\\text{LMMSE}} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y $\n",
    "\n",
    "Let's revisit the Unconstrained Minimum Mean Squared Error (MMSE) and the Maximum A Posteriori (MAP) estimators.\n",
    "\n",
    "1. **Unconstrained (Non-linear) MMSE Estimator:**\n",
    "\n",
    "The unconstrained MMSE estimator is obtained by minimizing the mean squared error without any constraints on the parameter. In the Bayesian framework, the unconstrained MMSE estimator is also known as the posterior mean, and it is given by the mean of the posterior distribution. In the case where $ \\theta $ follows a normal distribution, the posterior mean is equal to the posterior distribution's mean:\n",
    "\n",
    "$ \\hat{\\theta}_{\\text{MMSE}} = (C_{\\theta \\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y $\n",
    "\n",
    "This is the same expression as the LMMSE estimator in this case.\n",
    "\n",
    "2. **Maximum A Posteriori (MAP) Estimator:**\n",
    "\n",
    "The MAP estimator seeks the most probable value of the parameter given the observed data and the prior information. It is obtained by maximizing the posterior distribution:\n",
    "\n",
    "$ \\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} P(\\theta | Y) $\n",
    "\n",
    "In the case of a Gaussian prior, this is equivalent to minimizing the negative log posterior, and the MAP estimator is obtained as:\n",
    "\n",
    "$ \\hat{\\theta}_{\\text{MAP}} = \\arg \\min_{\\theta} \\left[ \\frac{1}{2} (Y - H \\theta)^T C_{VV}^{-1} (Y - H \\theta) + \\frac{1}{2} \\theta^T C_{\\theta \\theta}^{-1} \\theta \\right] $\n",
    "\n",
    "This minimization problem can be solved using optimization techniques. The solution is a compromise between fitting the observed data and staying close to the prior distribution.\n",
    "\n",
    "In summary, both the unconstrained MMSE and the MAP estimators in this case turn out to be the same as the LMMSE estimator due to the Gaussian assumption for the prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1c6c9-c915-461b-98f9-80cab57177c1",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(b)}$ What is the error covariance matrix ?\n",
    "$$\n",
    "R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE}= E_{\\theta} E_{Y|\\theta} \\tilde{\\theta}_{LMMSE}\\tilde{\\theta}^T_{LMMSE}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75eea5a-c663-4ba7-b2de-fe76b92750be",
   "metadata": {},
   "source": [
    "${\\color{Salmon}Response:}$\n",
    "\n",
    "Let's simplify the expression step by step. Recall the expression for the error covariance matrix $R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE}$:\n",
    "\n",
    "$ R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} = E_{\\theta} E_{Y|\\theta} (\\tilde{\\theta}_{LMMSE} - \\theta)(\\tilde{\\theta}_{LMMSE} - \\theta)^T $\n",
    "\n",
    "Now, substitute the expression for $\\tilde{\\theta}_{LMMSE}$:\n",
    "\n",
    "$ \\begin{split} R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} & = E_{\\theta} E_{Y|\\theta} \\left[(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y - (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} \\theta\\right] \\\\\n",
    "& \\qquad \\times \\left[Y^T C_{VV}^{-T} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} - \\theta^T C_{VV}^{-T} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1}\\right] \\end{split}$\n",
    "\n",
    "\n",
    "Let's simplify the terms inside the expectation. First, let's deal with the terms involving $Y$:\n",
    "\n",
    "$\\begin{split} & E_{\\theta} E_{Y|\\theta} \\left[(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y\\right] \\times \\left[Y^T C_{VV}^{-T} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1}\\right] \\\\\n",
    "& = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} E_{\\theta} E_{Y|\\theta} [Y Y^T] C_{VV}^{-T} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\\\\n",
    "& = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} H(C_{\\theta\\theta} + H^T C_{VV} H)H^T C_{VV}^{-1} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\\\\n",
    "& = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} H(C_{\\theta\\theta} + H^T C_{VV} H)C_{VV}^{-1} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\\\\n",
    "& = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} H(C_{\\theta\\theta} + H^T C_{VV} H)(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\end{split}$\n",
    "\n",
    "Now, let's deal with the terms involving $\\theta$:\n",
    "\n",
    "$ \\begin{split} & E_{\\theta} E_{Y|\\theta} \\left[(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} \\theta\\right] \\times \\left[\\theta^T C_{VV}^{-T} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1}\\right] \\\\\n",
    "& = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} E_{\\theta}[\\theta \\theta^T] C_{VV}^{-T} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\\\\n",
    "& = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} C_{\\theta\\theta} C_{VV}^{-1} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\\\\n",
    "& = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)C_{VV}^{-1} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\\\\n",
    "& = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} H(C_{\\theta\\theta} + H^T C_{VV} H)C_{VV}^{-1} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\end{split} $\n",
    "\n",
    "Now, combine these two results:\n",
    "\n",
    "$ \\begin{split} R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} & = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} H(C_{\\theta\\theta} + H^T C_{VV} H)C_{VV}^{-1} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\\\\n",
    "& \\quad - (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} H(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} \\end{split} $\n",
    "\n",
    "Now, simplify further by factoring out $(C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1}$:\n",
    "\n",
    "$ R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} H H^T C_{VV} C_{VV}^{-1} H (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} - (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} $\n",
    "\n",
    "Finally:\n",
    "\n",
    "$ R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV} C_{VV}^{-1} H (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} - (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} $\n",
    "\n",
    "This is the simplified expression for the error covariance matrix $R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE}$. The final form is dependent on the specific structure of the matrices $C_{\\theta\\theta}$, $C_{VV}$, and the design matrix $H$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641a558-4bf8-46a2-ad92-941e78dc1abc",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(c)}$ The conditional bias of an estimator $\\hat{\\theta}$ is $b_{\\hat{\\theta}}(\\theta) = E_{Y | \\theta} \\hat{\\theta}(Y) − \\theta.$\n",
    "The BLUE estimator is the LMMSE estimator under the constraint of conditional unbiasedness. So $b_{BLUE}(\\theta) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087798b5-ea8a-4c8f-b18f-902daef0b01f",
   "metadata": {},
   "source": [
    "- What is $\\hat{\\theta}_{BLUE}$ in terms of the quantities appearing in the Bayesian linear model  considered here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62356e16-763e-41c0-b556-ae4250a1b0a4",
   "metadata": {},
   "source": [
    "- Is there another classical deterministic estimator that equals $\\hat{\\theta}_{BLUE}$ in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610def5-f45b-4bc2-8d9d-a478c24925ee",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(d)}$ What is the error covariance matrix ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfdd4b-7026-44d0-84bb-93aaee87dd8c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation} \n",
    "R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE}= E_{\\theta} E_{Y|\\theta} \\tilde{\\theta}_{BLUE}\\tilde{\\theta}^T_{BLUE}\n",
    "\\end{equation} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197a3e7-2744-42e6-806b-df80ef055200",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(e)}$ Show that $R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} \\leq R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE}$\n",
    "Note that this is true in spite of $\\hat{\\theta}_{LMMSE}$ being (conditionally) biased and $\\hat{\\theta}_{BLUE}$ being unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5cedd3-8fa7-4df9-98dd-bef224e29b88",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(f)}$ Returning to $\\hat{\\theta}_{LMMSE}$, what is the bias $b_{LMMSE}(\\theta)$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4f6c4-8c04-4b50-82a2-d375310dd676",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(f)}$ Show that\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "R_{\\tilde{\\theta}\\tilde{\\theta}} = \\underline{E_{\\theta} b_{\\hat\\theta}(\\theta) b_{\\hat\\theta}^T(\\theta)} + \\underline{ E_{\\theta}E_{Y | \\theta} (\\hat{\\theta} - E_{Y|\\theta}\\hat{\\theta})(\\hat{\\theta} - E_{Y|\\theta}\\hat{\\theta})^T }\n",
    "\\\\\n",
    "\\widehat{(bias)^2}\\qquad \\qquad \\qquad \\widehat{variance}\\qquad\n",
    "\\end{gather}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef33c8e-9a1d-4f60-92bf-606fbbb0b011",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(h)}$  Compute $E_\\theta \\, b_{LMMSE}(\\theta) \\, b_{LMMSE}^T (\\theta) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc6a17-478a-4ed1-bedb-1b788cb054d4",
   "metadata": {},
   "source": [
    "#### ${\\color{Green}(i)}$   Compute $E_{\\theta} E_{Y|\\theta} (\\tilde{\\theta}_{LMMSE} - E_{Y|\\theta} \\tilde{\\theta}_{LMMSE} ) \\, (\\tilde{\\theta}_{LMMSE} - E_{Y|\\theta} \\tilde{\\theta}_{LMMSE})^T$ .\n",
    "Note that the sum of the positive definite matrices in ${\\color{Green}(h)}$ and ${\\color{Green}(i)}$ yields $R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE}$, for\n",
    "which ${\\color{Green}(e)}$  holds. Hence, in spite of the the fact that LMMSE introduces a (conditional) bias, it allows to reduce the variance so much that the sum of variance and squared bias gets lower than the variance in the unbiased case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dae447-acc4-4caa-9cfc-b4a4fea8a559",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da984b-2023-4b48-a22a-87479d6de4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
