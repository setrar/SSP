{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b71d937-8a49-43b6-859c-114b7dcbcd7f",
   "metadata": {},
   "source": [
    "# &#x1F4DD; REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e1cee-0b34-4540-825b-b9eded6a35aa",
   "metadata": {},
   "source": [
    "# Homework &#x0032;&#xFE0F;&#x20E3; \n",
    "\n",
    "Homework policy: the homework is individual. Students are encouraged to discuss with fellow students to try to find the main structure of the solution for a problem, especially if they are totally stuck at the beginning of the problem. However, you should work out the details yourself and write down in your own words only what you understand yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d04e5-cdf1-42f1-b3dc-5b9cc7e0b583",
   "metadata": {},
   "source": [
    "## Bayesian Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a21fe1-2ed8-413a-b2ac-0f47c871ce10",
   "metadata": {},
   "source": [
    "**&#x1F516;** **&#x0031;)** On the Beneficial Bias of MMSE Estimation\n",
    "\n",
    "Consider the Bayesian linear model $Y = H \\theta+V $ with $\\theta \\sim N(0,C_{\\theta\\theta})$  and $V \\sim N(0,C_{VV} )$ independent (we consider here $m_{\\theta} = 0$ for simplicity) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb8be5-7e3b-486f-905d-ec3496360965",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0061;) The LMMSE estimator is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28514370-095f-4f83-8211-af722a38eb32",
   "metadata": {},
   "source": [
    "$$\\hat{\\theta}_{LMMSE} = C_{\\theta Y} C_{YY}^{‚àí1}Y = (C_{\\theta\\theta}^{‚àí1} + H^T C_{VV}^{‚àí1} H)^{‚àí1} H^T C_{VV}^{‚àí1} Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268840cc-b023-4e57-b4b2-3ba6f0a72089",
   "metadata": {},
   "source": [
    "&#x1F516; What are the unconstrained (non-linear) MMSE and the MAP estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab7855-a2a9-45ac-a5e3-00fcb9216c9c",
   "metadata": {},
   "source": [
    "The Linear Minimum Mean Squared Error (LMMSE) estimator is given by:\n",
    "\n",
    "$$ \\hat{\\theta}_{\\text{LMMSE}} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y $$\n",
    "\n",
    "Let's revisit the Unconstrained Minimum Mean Squared Error (MMSE) and the Maximum A Posteriori (MAP) estimators.\n",
    "\n",
    "1. **Unconstrained (Non-linear) MMSE Estimator:**\n",
    "\n",
    "The unconstrained MMSE estimator is obtained by minimizing the mean squared error without any constraints on the parameter. In the Bayesian framework, the unconstrained MMSE estimator is also known as the posterior mean, and it is given by the mean of the posterior distribution. In the case where $ \\theta $ follows a normal distribution, the posterior mean is equal to the posterior distribution's mean:\n",
    "\n",
    "$$ \\hat{\\theta}_{\\text{MMSE}} = (C_{\\theta \\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y $$\n",
    "\n",
    "This is the same expression as the LMMSE estimator in this case.\n",
    "\n",
    "2. **Maximum A Posteriori (MAP) Estimator:**\n",
    "\n",
    "The MAP estimator seeks the most probable value of the parameter given the observed data and the prior information. It is obtained by maximizing the posterior distribution:\n",
    "\n",
    "$$ \\hat{\\theta}_{\\text{MAP}} = \\arg \\max_{\\theta} P(\\theta | Y) $$\n",
    "\n",
    "In the case of a Gaussian prior, this is equivalent to minimizing the negative log posterior, and the MAP estimator is obtained as:\n",
    "\n",
    "$$ \\hat{\\theta}_{\\text{MAP}} = \\arg \\min_{\\theta} \\left[ \\frac{1}{2} (Y - H \\theta)^T C_{VV}^{-1} (Y - H \\theta) + \\frac{1}{2} \\theta^T C_{\\theta \\theta}^{-1} \\theta \\right] $$\n",
    "\n",
    "This minimization problem can be solved using optimization techniques. The solution is a compromise between fitting the observed data and staying close to the prior distribution.\n",
    "\n",
    "In summary, both the unconstrained MMSE and the MAP estimators in this case turn out to be the same as the LMMSE estimator due to the Gaussian assumption for the prior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2bd1c-8680-471b-93d5-2c2c714576fc",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0062;) What is the error covariance matrix ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6267dfe4-9e0c-48cb-8865-f31c7a5cc19d",
   "metadata": {},
   "source": [
    "$$ R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE}= E_{\\theta} E_{Y|\\theta} \\tilde{\\theta}_{LMMSE}\\tilde{\\theta}^T_{LMMSE}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381ca36-ba21-46cd-88ba-98f79b7fc811",
   "metadata": {},
   "source": [
    "The error covariance matrix $ R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} $ in the context of Linear Minimum Mean Square Error (LMMSE) estimation represents the covariance of the estimation error of the parameter vector $ \\tilde{\\theta} $. This matrix quantifies the accuracy of the LMMSE estimator by describing how the estimation errors for the parameters are correlated with each other. The error covariance matrix is crucial in signal processing and estimation theory, as it provides insight into the performance and reliability of the estimator.\n",
    "\n",
    "However, the provided equation seems to have a misunderstanding in its notation and formulation. Typically, the error covariance matrix for an LMMSE estimator is derived from the difference between the actual parameter values and their estimates, not directly as a product of expectations involving the estimates themselves.\n",
    "\n",
    "The correct expression for the error covariance matrix of the LMMSE estimator, $ R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} $, is generally given by:\n",
    "\n",
    "$$R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} = E\\left[ (\\theta - \\tilde{\\theta}_{LMMSE})(\\theta - \\tilde{\\theta}_{LMMSE})^T \\right]$$\n",
    "\n",
    "where: \n",
    "- a) $\\theta$ is the true parameter vector. \n",
    "- b) $\\tilde{\\theta}_{LMMSE}$ is the LMMSE estimate of $\\theta$. \n",
    "- c) $E[\\cdot]$ denotes the expectation operator.\n",
    "\n",
    "In the context of linear models, where $ \\theta $ is estimated from observations $ Y $ related through $ Y = H\\theta + V $ with $ H $ being the observation matrix and $ V $ representing the noise, the LMMSE estimator and its error covariance matrix are derived based on the statistics of $ \\theta $ and $ V $. Specifically, the error covariance matrix can be expressed using the model parameters and noise characteristics:\n",
    "\n",
    "$$R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1}$$\n",
    "\n",
    "Here: \n",
    "- a) $C_{\\theta\\theta}$ is the covariance matrix of $\\theta$.\n",
    "- b) $C_{VV}$ is the covariance matrix of the noise $V$.\n",
    "- c) $H^T$ is the transpose of the observation matrix $H$.\n",
    "\n",
    "This formulation captures the essence of how the error covariance matrix quantifies the uncertainty or accuracy of the LMMSE estimates, reflecting how the estimation errors are expected to covary across the estimated parameters.\n",
    "\n",
    "* See Some Optimal Properties (2) in Course Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f508b38-d89a-4a12-a5f5-f8fe5527d2e1",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0063;) The conditional bias of an estimator $\\hat{\\theta}$ is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33fcff5-c3a0-4c0a-8cd1-1446c7907212",
   "metadata": {},
   "source": [
    "$$b_{\\hat{\\theta}}(\\theta) = E_{Y | \\theta} \\hat{\\theta}(Y) ‚àí \\theta$$\n",
    "\n",
    "The BLUE estimator is the LMMSE estimator under the constraint of conditional unbiasedness. So $ b_{BLUE}(\\theta) = 0$.\n",
    "\n",
    "What is $\\hat{\\theta}_{BLUE}$ in terms of the quantities appearing in the Bayesian linear model considered here?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14293f2-5632-4334-865a-6f43b9d3d12c",
   "metadata": {},
   "source": [
    "In the Bayesian linear model considered here, the BLUE (Best Linear Unbiased Estimator) for the parameter vector $\\theta$ is given by:\n",
    "\n",
    "$$\\hat{\\theta}_{\\text{BLUE}} = (H^T C_{VV}^{-1} H + C_{\\theta\\theta}^{-1})^{-1} H^T C_{VV}^{-1} Y $$\n",
    "\n",
    "Let's break down the terms:\n",
    "\n",
    "- let $Y$: Observed data vector.\n",
    "- let $H$: Design matrix.\n",
    "- let $C_{VV}$: Covariance matrix of the observation noise $V$.\n",
    "- let $C_{\\theta\\theta}$: Covariance matrix of the prior distribution for $\\theta$.\n",
    "\n",
    "So, in terms of the quantities appearing in the Bayesian linear model:\n",
    "\n",
    "1. here: $H^T C_{VV}^{-1} H$: This term reflects the information provided by the observed data and how well it explains the variations in $\\theta$. It is weighted by the inverse covariance of the observation noise.\n",
    "\n",
    "2. here: $C_{\\theta\\theta}^{-1}$: This term reflects the precision of the prior information about $\\theta$. It is the inverse of the covariance matrix of the prior distribution for $\\theta$.\n",
    "\n",
    "3. here: $H^T C_{VV}^{-1} Y$: This term combines the information from the observed data with the inverse covariance of the observation noise.\n",
    "\n",
    "4. The entire expression $(H^T C_{VV}^{-1} H + C_{\\theta\\theta}^{-1})^{-1}$: This is the inverse of the sum of the information from the observed data and the precision of the prior distribution. It represents the combined information for estimating $\\theta$ in a linear, unbiased, and efficient manner.\n",
    "\n",
    "So, the BLUE estimator $\\hat{\\theta}_{\\text{BLUE}}$ balances the information from the observed data and the prior distribution, providing an optimal linear estimator for $\\theta$ in terms of unbiasedness and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef17a4-2c1b-4b27-a9cb-c137ef178582",
   "metadata": {},
   "source": [
    "&#x1F516; Is there another classical deterministic estimator that equals $\\hat{\\theta}_{BLUE}$ in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc040c-e517-4132-a53b-8ff457dfad57",
   "metadata": {},
   "source": [
    "Yes, the Ordinary Least Squares (OLS) estimator equals the Best Linear Unbiased Estimator (BLUE) under certain conditions, specifically when the linear model's errors are homoscedastic (constant variance), uncorrelated, and the model is correctly specified. In such cases, OLS is not only unbiased but also achieves the minimum variance among all linear estimators, making it equivalent to BLUE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb823447-67c2-48b3-ae0e-d3085a2060bf",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0064;) What is the error covariance matrix ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65c674-f210-42b5-a5c2-26927370ed23",
   "metadata": {},
   "source": [
    "$$ R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE}= E_{\\theta} E_{Y|\\theta} \\tilde{\\theta}_{BLUE}\\tilde{\\theta}^T_{BLUE}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89beceb7-e70a-4f79-bf46-9762cd28c4f8",
   "metadata": {},
   "source": [
    "The provided equation for $ R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE} $ appears to have a notational misunderstanding regarding the definition and computation of the error covariance matrix for the Best Linear Unbiased Estimator (BLUE). Let's clarify the correct formulation and meaning of this matrix.\n",
    "\n",
    "* Correct Formulation of the BLUE Error Covariance Matrix\n",
    "\n",
    "The error covariance matrix $ R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE} $ for the Best Linear Unbiased Estimator (BLUE) is not computed via the expectation of products of estimates conditioned on $\\theta$ and $Y$. Instead, it's defined based on the properties of the estimation error. The correct definition is:\n",
    "\n",
    "$$R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE} = E\\left[ (\\tilde{\\theta}_{BLUE} - \\theta)(\\tilde{\\theta}_{BLUE} - \\theta)^T \\right]$$\n",
    "\n",
    "Here, $ \\tilde{\\theta}_{BLUE} $ is the BLUE of the parameter vector $ \\theta $, and the expectation $ E[\\cdot] $ is taken over the distribution of the data $ Y $ given $ \\theta $, reflecting all possible outcomes and their probabilities.\n",
    "\n",
    "* Understanding $R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE}$\n",
    "\n",
    "- Here, $R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE}$ quantifies the variance and covariance of the estimation errors for the BLUE. It tells us how much the estimates $\\tilde{\\theta}_{BLUE} $ are expected to deviate from the true parameter values $ \\theta $, and how these deviations are correlated between different elements of $\\theta$.\n",
    "\n",
    "- The BLUE is derived under the assumptions that the estimators are linear functions of the data and that they are unbiased. The Gauss-Markov theorem assures us that the BLUE has the smallest variance (in a matrix sense) among all linear unbiased estimators of $ \\theta $.\n",
    "\n",
    "How to Compute $ R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE} $\n",
    "\n",
    "In a linear model $ Y = H\\theta + V $, where $ Y $ is the observed data, $ H $ is a known matrix, $ \\theta $ is the parameter vector to be estimated, and $ V $ is the noise with covariance matrix $ C_{VV} $, the BLUE of $ \\theta $ is given by:\n",
    "\n",
    "$$\\tilde{\\theta}_{BLUE} = (H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y$$\n",
    "\n",
    "And its error covariance matrix is:\n",
    "\n",
    "$$R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE} = (H^T C_{VV}^{-1} H)^{-1}$$\n",
    "\n",
    "This matrix $ R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE} $ directly arises from the linear estimation theory and reflects the influence of both the data's relationship to the parameters (via $ H $) and the noise characteristics (via $ C_{VV} $).\n",
    "\n",
    "In summary, the correct formulation for $ R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE} $ is based on the variance and covariance of the estimation errors, derived from the linear model assumptions and the properties of the noise, not from the expectation of the product of conditional estimates as initially stated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b93a7-ae4e-4255-a802-5082ff690fa1",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0065;) Show that $R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} \\leq R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d968b-ca80-4087-a58f-9c8eda3733a1",
   "metadata": {},
   "source": [
    "The statement $R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE} \\leq R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE}$ suggests that the error covariance matrix for the Linear Minimum Mean Square Error (LMMSE) estimator is equal to or smaller than that of the Best Linear Unbiased Estimator (BLUE) under certain conditions. However, this comparison needs clarification because:\n",
    "\n",
    "- The LMMSE estimator minimizes the mean squared error, which includes both bias and variance, and can be biased or unbiased.\n",
    "- The BLUE guarantees the minimum variance among all linear unbiased estimators without necessarily minimizing the overall mean squared error.\n",
    "\n",
    "Without additional context, comparing $R_{\\tilde{\\theta}\\tilde{\\theta}}^{LMMSE}$ and $R_{\\tilde{\\theta}\\tilde{\\theta}}^{BLUE}$ directly is not straightforward. If the LMMSE estimator is unbiased in a given scenario, it coincides with the BLUE, making their error covariance matrices equal. Otherwise, LMMSE focuses on minimizing the MSE, potentially accepting bias for a lower overall error, which is a different optimization criterion than that of BLUE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a715abe-c4c8-43d1-91ca-eb68251de113",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0066;) Returning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ebaf47-de50-4fc8-810e-495f6196625d",
   "metadata": {},
   "source": [
    "to $\\hat{\\theta}_{\\text{LMMSE}}$, what is the bias $b_{\\text{LMMSE}}(\\theta)$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e6f5b-2671-4027-8ddc-1807d17d55aa",
   "metadata": {},
   "source": [
    "The bias of an estimator is a measure of systematic error in estimation, defined as the difference between the expected value of the estimator and the true value of the parameter being estimated. For the Linear Minimum Mean Square Error (LMMSE) estimator in the context of the Bayesian linear model $Y = H\\theta + V$, where $\\theta \\sim N(0, C_{\\theta\\theta})$ and $V \\sim N(0, C_{VV})$ are independent, the bias of the LMMSE estimator, denoted as $b_{LMMSE}(\\theta)$, can be defined and evaluated.\n",
    "\n",
    "* Definition of Bias\n",
    "\n",
    "For an estimator $\\tilde{\\theta}$ of the parameter $\\theta$, the bias is given by:\n",
    "\n",
    "$$b_{\\tilde{\\theta}}(\\theta) = E[\\tilde{\\theta}] - \\theta$$\n",
    "\n",
    "Where $E[\\tilde{\\theta}]$ is the expected value of the estimator.\n",
    "\n",
    "* Bias of the LMMSE Estimator\n",
    "\n",
    "Given the Bayesian setting and assuming a Gaussian prior for $\\theta$ and Gaussian noise $V$, the LMMSE estimator is designed to minimize the mean squared error, incorporating both variance and bias. Specifically, the LMMSE estimator for $\\theta$ can be biased unless the prior mean $m_{\\theta}$ coincides with the true parameter value $\\theta$, which is not generally assumed outside of specific contexts (such as zero-mean priors matching the true parameter mean).\n",
    "\n",
    "However, in the provided model setup, with $m_{\\theta} = 0$, the bias of the LMMSE estimator is inherently linked to how well the prior distribution aligns with the true distribution of $\\theta$. If $m_{\\theta} = 0$ accurately reflects the true mean of $\\theta$, the bias for estimating $\\theta$ itself could be minimized or potentially be zero, particularly if the model is correctly specified and the assumptions hold true.\n",
    "\n",
    "* Computing the Bias\n",
    "\n",
    "For the specific LMMSE estimator \n",
    "\n",
    "$$\n",
    "\\tilde{\\theta}_{LMMSE} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y\n",
    "$$\n",
    "\n",
    "the expected value $E[\\tilde{\\theta}_{LMMSE}]$ considering the distribution of $Y$ given $\\theta$ and the noise characteristics can be evaluated to find the bias. \n",
    "\n",
    "In this specific setup with $m_{\\theta} = 0$, and if $\\theta$ is truly centered at zero, the LMMSE estimator is designed to be unbiased for $\\theta$ itself under the model assumptions. However, the presence of $C_{\\theta\\theta}$ in the estimator formulation indicates that any deviation of the prior assumptions from the true parameter characteristics could introduce bias. \n",
    "\n",
    "* Conclusion\n",
    "\n",
    "The bias $b_{LMMSE}(\\theta)$ for the LMMSE estimator in this context depends on the alignment between the prior assumptions (especially the mean $m_{\\theta}$) and the true characteristics of $\\theta$. If the prior accurately reflects the true distribution, the LMMSE estimator can have minimal bias, but deviations in the prior from the true parameter distribution can introduce systematic errors in estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755d8b4f-d3b1-451e-ae31-24b3742bce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9accc352-37a1-433c-b0ad-78f1ddedc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given values\n",
    "ùêª = [1 2]         # Design matrix, transposed to match the formula\n",
    "ùê∂‚ÇÄ‚ÇÄ = 4           # Variance of theta\n",
    "ùê∂·µ•·µ• = [1 0; 0 1]  # Covariance matrix of the observation noise\n",
    "ùëå = [3, 7];       # Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb44c25b-d59b-4761-92f4-a8590b84319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using FFTW, Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0784c31-97ad-4c28-8d17-65176108d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../modules/operations.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d597a7f5-b399-4590-be13-dfaa5f7d5624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ùê∂·µ•·µ•‚Åª¬π = [1.0 0.0; 0.0 1.0]\n"
     ]
    }
   ],
   "source": [
    "ùê∂‚ÇÄ‚ÇÄ‚Åª¬π    = 1/ùê∂‚ÇÄ‚ÇÄ                      # also (ùê∂‚ÇÄ‚ÇÄ)‚Åª¬π Inverse of C_theta_theta scalar in this case\n",
    "ùê∂·µ•·µ•‚Åª¬π    = (ùê∂·µ•·µ•)‚Åª¬π; @show  ùê∂·µ•·µ•‚Åª¬π      # Inverse of C_VV, which is the identity matrix in this case\n",
    "ùêª·µÄùê∂·µ•·µ•‚Åª¬πùêª = (ùêª)·µÄ.*ùê∂·µ•·µ•‚Åª¬π.* ùêª           # Compute H^T C_VV^{-1} H \n",
    "ùêª·µÄùê∂·µ•·µ•‚Åª¬πùëå = (ùêª)·µÄ.*ùê∂·µ•·µ•‚Åª¬π * ùëå;          # Compute H^T C_VV^{-1} Y (H is transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a51df76-6387-4cf5-b0bf-b07422008b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01294b13-78c3-46d8-a86a-1dbe0a003f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\paragraph{\\&\\#x1F4DD; Calculate $\\hat{\\theta}_{LMMSE}$ using the formula:}\n",
       "$$\\hat{\\theta}_{LMMSE} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y$$\n"
      ],
      "text/markdown": [
       "#### &#x1F4DD; Calculate $\\hat{\\theta}_{LMMSE}$ using the formula:\n",
       "\n",
       "$$\n",
       "\\hat{\\theta}_{LMMSE} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y\n",
       "$$\n"
      ],
      "text/plain": [
       "\u001b[1m  &#x1F4DD; Calculate \u001b[35m\\hat{\\theta}_{LMMSE}\u001b[39m using the formula:\u001b[22m\n",
       "\u001b[1m  -----------------------------------------------------------\u001b[22m\n",
       "\n",
       "\u001b[35m  \\hat{\\theta}_{LMMSE} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown.parse(raw\"\n",
    "#### &#x1F4DD; Calculate $\\hat{\\theta}_{LMMSE}$ using the formula:\n",
    "```math\n",
    "\\hat{\\theta}_{LMMSE} = (C_{\\theta\\theta}^{-1} + H^T C_{VV}^{-1} H)^{-1} H^T C_{VV}^{-1} Y\n",
    "```\n",
    "\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4df21932-37df-4288-a09a-2da33be155f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "Œ∏ÃÇ‚Çó‚Çò‚Çò‚Çõ‚Çë = (ùê∂‚ÇÄ‚ÇÄ‚Åª¬π.+ùêª·µÄùê∂·µ•·µ•‚Åª¬πùêª)‚Åª¬π*ùêª·µÄùê∂·µ•·µ•‚Åª¬πùëå # Compute theta_LMMSE\n",
    "\n",
    "println(round.(Œ∏ÃÇ‚Çó‚Çò‚Çò‚Çõ‚Çë))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6134c7-fc22-4a47-8afb-44369bb71fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ùëÅ = Normal;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed01483-a781-44a5-9dd5-447b9cddadfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated theta: [5.61;;]\n",
      "Bias of the estimator: [0.61;;]\n"
     ]
    }
   ],
   "source": [
    "Œ∏ÃÉ = 5 # True parameter value\n",
    "ùê∂ÃÉ·µ•·µ• = 1\n",
    "\n",
    "# Generate observation Y = H*theta_true + V\n",
    "ùëâ = rand(ùëÅ(0, ‚àö(ùê∂ÃÉ·µ•·µ•)))  # Generate noise V\n",
    "ùëåÃÉ = ùêª * Œ∏ÃÉ .+ ùëâ  # Single observation\n",
    "\n",
    "# Simplified estimate of theta (not using Bayesian posterior for simplicity)\n",
    "# Assume a simple estimator like the least squares estimate for demonstration\n",
    "Œ∏ÃÇ = ùëåÃÉ / ùêª\n",
    "\n",
    "# Calculate bias (not strictly Bayesian, but for demonstration)\n",
    "ùëè‚ÇÄ = Œ∏ÃÇ .- Œ∏ÃÉ\n",
    "\n",
    "println(\"Estimated theta: \", round.(Œ∏ÃÇ, digits = 2))\n",
    "println(\"Bias of the estimator: \", round.(ùëè‚ÇÄ, digits = 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a386c5-1347-4e41-a034-3de51d24d5e8",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0067;) Show that "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebe094-6664-4ed1-969f-f4c752131799",
   "metadata": {},
   "source": [
    "$$R_{\\tilde{\\theta}\\tilde{\\theta}} = \\frac{E_{\\theta} b_{\\hat\\theta}(\\theta) b_{\\hat\\theta}^T(\\theta)}{\\widehat{(bias)^2}} + \\frac{E_{\\theta}E_{Y | \\theta} (\\hat{\\theta} - E_{Y|\\theta}\\hat{\\theta})(\\hat{\\theta} - E_{Y|\\theta}\\hat{\\theta})^T }{\\widehat{variance}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede32698-0284-4dab-b525-8b6494e4d67c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d975c5-38d9-47e9-9d51-a3a0f7d5ab5e",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0067;) Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fcd68-66bb-4b10-bd46-e9fc1dd0e9eb",
   "metadata": {},
   "source": [
    "$$E_\\theta \\, b_{LMMSE}(\\theta) \\, b_{LMMSE}^T (\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3073555-fbcb-4d9b-bc94-98dd13a49d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a868880b-c865-40d5-8e9d-2242d00fb46d",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0068;) Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af5d9e-1310-4a1e-a14a-cd4f449b1ff0",
   "metadata": {},
   "source": [
    "$$E_{\\theta} E_{Y|\\theta} (\\tilde{\\theta}_{LMMSE} - E_{Y|\\theta} \\tilde{\\theta}_{LMMSE} ) \\, (\\tilde{\\theta}_{LMMSE} - E_{Y|\\theta} \\tilde{\\theta}_{LMMSE})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228aff76-58be-4060-87c2-0c8b2e7ab578",
   "metadata": {},
   "source": [
    "Note that the sum of the positive definite matrices in (h) and (i) yields $R_{LMMSE}$, for\n",
    "which (e) holds. Hence, in spite of the the fact that LMMSE introduces a (conditional) bias, it allows to reduce the variance so much that the sum of variance and squared bias gets lower than the variance in the unbiased case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba07e6-1477-4d5b-8fb8-3faea32ab6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc94ee35-cc89-4f8a-8651-efcb11c2abb5",
   "metadata": {},
   "source": [
    "## Deterministic Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113eb746-a4ee-45a3-9a51-f204715f1479",
   "metadata": {},
   "source": [
    "**&#x1F516;** **&#x0032;)** ML Estimation of Roundtrip Delay Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b47db2-da94-4224-a9c9-4efb68dd33f3",
   "metadata": {},
   "source": [
    "Assume that for the roundtrip delay in a computer network, as considered in the homework, we now consider a truncated exponential distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6b99c-43f2-4dd1-9253-4602ee7a7b48",
   "metadata": {},
   "source": [
    "$$\n",
    "f(y|\\lambda,\\alpha,\\beta) = \n",
    "\\left\\{\\!\\begin{aligned}\n",
    "&0 &, y < \\alpha \\\\\n",
    "&\\gamma e ^{-\\lambda y} &, \\alpha \\leq y \\leq \\beta   \\\\\n",
    "&0 &, \\beta < y\n",
    "\\end{aligned}\\right\\} \\\\\n",
    "= \\gamma e ^{-\\lambda y} 1_{[\\alpha,\\beta]} (y)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is a normalization constant and\n",
    "\n",
    "$$\n",
    "1_\\mathcal{A}(y) =\n",
    "\\begin{cases}\n",
    "&1 &, y \\in \\mathcal{A} \\\\\n",
    "&0 &, y \\notin \\mathcal{A}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "is the indicator function for the set $\\mathcal{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048bc2c-e818-4285-be44-e127465626c3",
   "metadata": {},
   "source": [
    "&#x1F516; (&#x0061;) Determine the normalization constant $\\gamma$ as a function of $\\lambda$, $\\alpha$ and $\\beta$.\n",
    "\n",
    "In what follows, you substitute $\\gamma$ in $f(y|\\lambda,\\alpha,\\beta)$ by this function of $\\lambda$, $\\alpha$ and $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1915b-f156-4dc5-8af1-c920116d314e",
   "metadata": {},
   "source": [
    "Given:\n",
    "\n",
    "$$\n",
    "f(y|\\lambda,\\alpha,\\beta) = \\gamma e^{-\\lambda y} 1_{[\\alpha,\\beta]}(y)\n",
    "$$\n",
    "\n",
    "To find $\\gamma$, we solve:\n",
    "\n",
    "$$\n",
    "\\int_{\\alpha}^{\\beta} \\gamma e^{-\\lambda y} dy = 1\n",
    "$$\n",
    "\n",
    "Performing the integration:\n",
    "\n",
    "$$\n",
    "\\int_{\\alpha}^{\\beta} \\gamma e^{-\\lambda y} dy = \\left[ -\\frac{\\gamma}{\\lambda} e^{-\\lambda y} \\right]_{\\alpha}^{\\beta}\n",
    "$$\n",
    "\n",
    "$$ = -\\frac{\\gamma}{\\lambda} \\left( e^{-\\lambda \\beta} - e^{-\\lambda \\alpha} \\right) $$\n",
    "\n",
    "$$ = \\frac{\\gamma}{\\lambda} \\left( e^{-\\lambda \\alpha} - e^{-\\lambda \\beta} \\right) = 1 $$\n",
    "\n",
    "Solving for $\\gamma$, we find:\n",
    "\n",
    "$$ \\gamma = \\frac{\\lambda}{e^{-\\lambda \\alpha} - e^{-\\lambda \\beta}} $$\n",
    "\n",
    "This $\\gamma$ ensures that the pdf of the truncated exponential distribution integrates to 1 over the interval $[\\alpha, \\beta]$, making it a valid probability distribution.\n",
    "\n",
    "The presence of the indicator function $1_{[\\alpha,\\beta]}(y)$ in the definition of the pdf explicitly enforces that the distribution is zero outside the interval $[\\alpha, \\beta]$, ensuring the distribution is properly truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd691f5c-996b-4566-9e5a-022d1549b218",
   "metadata": {},
   "source": [
    " &#x1F516; (&#x0061;) We now collect n i.i.d. measurements $y_i$ into the vector $Y$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecf9bc4-b776-4f71-823b-b16fc3bbab20",
   "metadata": {},
   "source": [
    "Assume for the moment that $\\lambda > 0$ is a given constant.\n",
    "\n",
    "Find the likelihood function $l(\\alpha, \\beta|Y, \\lambda)$ for $\\alpha$ and $\\beta$ given $Y$ and $\\lambda$\n",
    "\n",
    "Note that $1_{[\\alpha,\\beta]}(y)=1_{[\\alpha,\\infty]}(y)1_{[-\\infty,\\beta]}(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b4aa6-1f2b-45d9-931f-80f6b4b62f7c",
   "metadata": {},
   "source": [
    "Given $\\lambda > 0$ as a constant and a set of observations $Y = \\{y_1, y_2, \\ldots, y_n\\}$ from a truncated exponential distribution, the likelihood function $l(\\alpha, \\beta | Y, \\lambda)$ for estimating the parameters $\\alpha$ and $\\beta$ can be derived from the probability density function:\n",
    "\n",
    "$$ f(y|\\lambda,\\alpha,\\beta) = \\gamma e^{-\\lambda y} \\mathbf{1}_{[\\alpha,\\beta]}(y) $$\n",
    "\n",
    "where $\\gamma = \\frac{\\lambda e^{\\lambda \\alpha}}{e^{\\lambda \\alpha} - e^{\\lambda \\beta}}$ ensures normalization.\n",
    "\n",
    "Given that $1_{[\\alpha,\\beta]}(y) = 1_{[\\alpha,\\infty]}(y)1_{[-\\infty,\\beta]}(y)$, the likelihood of observing $Y$ given $\\alpha$, $\\beta$, and $\\lambda$ is the product of the densities of each observation:\n",
    "\n",
    "$$ l(\\alpha, \\beta|Y, \\lambda) = \\prod_{i=1}^n f(y_i|\\lambda,\\alpha,\\beta) $$\n",
    "\n",
    "Substituting $f(y|\\lambda,\\alpha,\\beta)$ into the likelihood function:\n",
    "\n",
    "$$ l(\\alpha, \\beta|Y, \\lambda) = \\prod_{i=1}^n \\left( \\gamma e^{-\\lambda y_i} \\mathbf{1}_{[\\alpha,\\infty]}(y_i)\\mathbf{1}_{[-\\infty,\\beta]}(y_i) \\right) $$\n",
    "\n",
    "Since $\\gamma$ and $\\lambda$ are constants for all $y_i$ and the indicator function ensures that only values within $[\\alpha, \\beta]$ contribute to the likelihood, this simplifies to:\n",
    "\n",
    "$$ l(\\alpha, \\beta|Y, \\lambda) = \\gamma^n e^{-\\lambda \\sum_{i=1}^n y_i} \\prod_{i=1}^n \\mathbf{1}_{[\\alpha,\\infty]}(y_i)\\mathbf{1}_{[-\\infty,\\beta]}(y_i) $$\n",
    "\n",
    "Given the expression for $\\gamma$, the likelihood function explicitly depends on $\\alpha$ and $\\beta$ through $\\gamma$ and the indicator functions, which ensure all $y_i$ fall within $[\\alpha, \\beta]$. The product of indicator functions is 1 if all $y_i$ are within the interval $[\\alpha, \\beta]$ and 0 otherwise. \n",
    "\n",
    "This forms the basis for the likelihood function $l(\\alpha, \\beta | Y, \\lambda)$, which is maximized to find estimates for $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3dcd0-89bd-4467-9a2e-6bdabfddfcf3",
   "metadata": {},
   "source": [
    "# &#x1F4DA; References\n",
    "- [ ] [Bayesian Linear Regression : Data Science Concepts](https://www.youtube.com/watch?v=Z6HGJMUakmc&t=49s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21fad9-7c19-4ff5-ad14-128d9372cde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
