{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece7842a-3672-4ba4-b826-c9635fe43419",
   "metadata": {},
   "source": [
    "**&#x1F516;** **&#x0031;)** On the Beneficial Bias of MMSE Estimation\n",
    "\n",
    "Consider the Bayesian linear model $Y = H \\theta+V $ with $\\theta \\sim N(0,C_{\\theta\\theta})$  and $V \\sim N(0,C_{VV} )$ independent (we consider here $m_{\\theta} = 0$ for simplicity) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76281985-5bcf-43a9-9050-d4d66bc2d204",
   "metadata": {},
   "source": [
    "Given the Bayesian linear model you've described, we have an output $Y$ that is modeled as a linear function of some input variables collected in matrix $H$, weighted by parameters $\\theta$, with additive noise $V$. The parameters $\\theta$ and the noise $V$ are both assumed to be normally distributed, with $\\theta$ having a mean of $0$ (denoted as $m_{\\theta} = 0$ for simplicity) and covariance matrix $C_{\\theta\\theta}$, and $V$ also having a mean of $0$ and covariance matrix $C_{VV}$. Furthermore, $\\theta$ and $V$ are assumed to be independent of each other.\n",
    "\n",
    "This setup allows for the derivation of predictions for $Y$ given new inputs $H$ and the computation of uncertainties associated with these predictions. The independence between $\\theta$ and $V$ simplifies the analysis significantly.\n",
    "\n",
    "To proceed with such a model, one typically wants to:\n",
    "1. **Predict the output $Y$ for new inputs $H$:** This involves computing the mean and variance of the predictive distribution of $Y$ given new $H$.\n",
    "2. **Estimate the parameters $\\theta$:** Although $\\theta$ has a prior distribution $N(0, C_{\\theta\\theta})$, we update our beliefs about $\\theta$ after observing data. This is achieved through Bayesian updating, leading to a posterior distribution of $\\theta$ given the observed data.\n",
    "\n",
    "### Predictive Distribution of $Y$\n",
    "\n",
    "Given $Y = H\\theta + V$, the mean and covariance of $Y$ can be derived from the properties of linear transformations of Gaussian distributions. The mean of $Y$ is given by the expected value of the linear transformation, which, due to $m_{\\theta} = 0$, simplifies to:\n",
    "\n",
    "$\n",
    "E[Y] = H E[\\theta] + E[V] = H \\cdot 0 + 0 = 0\n",
    "$\n",
    "\n",
    "The covariance of $Y$, denoted as $C_{YY}$, takes into account both the variance introduced by the parameters $\\theta$ and the observational noise $V$, and can be computed as:\n",
    "\n",
    "$\n",
    "C_{YY} = H C_{\\theta\\theta} H^T + C_{VV}\n",
    "$\n",
    "\n",
    "### Posterior Distribution of $\\theta$\n",
    "\n",
    "Given observations of $Y$ and known $H$, one can compute the posterior distribution of $\\theta$, which represents our updated beliefs about $\\theta$ after observing data. This is where Bayesian inference comes into play, updating the prior distribution $N(0, C_{\\theta\\theta})$ to a posterior distribution that reflects the observed data.\n",
    "\n",
    "The posterior distribution of $\\theta$ can be calculated using Bayes' theorem, which, for linear Gaussian models, results in another Gaussian distribution whose mean and covariance can be derived analytically through the normal equations or by applying the Kalman filter update equations in the context of state space models.\n",
    "\n",
    "Would you like to delve into any specific calculations or further aspects of this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202671b6-a09d-453c-ae2e-e2b0e34a4f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
