{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a055bc05-10ef-44ff-8adc-3ef19faf6b1d",
   "metadata": {},
   "source": [
    "# Eigendecomposition Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df19cc5-2b54-4d92-8251-a261a391bcfe",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{gather}\n",
    "\\mathit{ eigenvalues } \\space \\lambda_i \\text{ and corresponding } \\mathit{ eigenvectors } \\space V_i \\text{ of } C_{XX} : C_{XX} V_i = \\lambda_i V_i\n",
    "\\\\\n",
    "\\text{ fix } \\mathit{ norm } \\space \\| V_i \\| = 1, \\| V_i \\|^2 = V_i^TV_i\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea981de-3609-41fa-b0bf-211cc4137918",
   "metadata": {},
   "source": [
    "## How  to calculate the Eigendecomposition Covariance Matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38608378-d487-48a2-8590-b71c1fa46b1a",
   "metadata": {},
   "source": [
    "To calculate the eigendecomposition of a covariance matrix, you can follow these steps:\n",
    "\n",
    "**Step 1: Compute the Covariance Matrix**\n",
    "\n",
    "First, calculate the covariance matrix from your data. If you have a dataset with n observations and p variables, the covariance matrix (often denoted as Σ) is a p × p square matrix. The element in the i-th row and j-th column of the covariance matrix represents the covariance between variables i and j. You can calculate it using the following formula:\n",
    "\n",
    "$$ \\Sigma = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(X_i - \\bar{X})^T $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\text{ - n is the number of observations. } \\\\\n",
    "- X_i \\text { represents the i-th data vector. } \\\\\n",
    "- \\bar{X} \\text{  is the mean vector of the data. }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "**Step 2: Calculate the Eigenvectors and Eigenvalues**\n",
    "\n",
    "Once you have the covariance matrix, you need to find its eigenvectors and eigenvalues. The eigenvectors represent the directions along which the data varies the most (principal components), and the eigenvalues represent the variance along those directions.\n",
    "\n",
    "You can find the eigenvectors and eigenvalues by solving the eigenvalue problem:\n",
    "\n",
    "$$ \\Sigma v = \\lambda v $$\n",
    "\n",
    "Where:\n",
    "- Σ is the covariance matrix.\n",
    "- \\(v\\) is the eigenvector.\n",
    "- λ is the eigenvalue.\n",
    "\n",
    "This equation can be solved using mathematical software or libraries like NumPy in Python. The result will be a set of p eigenvectors and p corresponding eigenvalues.\n",
    "\n",
    "**Step 3: Arrange Eigenvectors and Eigenvalues**\n",
    "\n",
    "Order the eigenvectors in decreasing order of their eigenvalues. This means that the first eigenvector corresponds to the direction of maximum variance, the second eigenvector corresponds to the second-largest variance, and so on. This reordering is essential to identify the principal components.\n",
    "\n",
    "**Step 4: Form the Eigenvector Matrix**\n",
    "\n",
    "Create a matrix, often denoted as P, where each column is one of the ordered eigenvectors. This matrix is sometimes called the \"eigenvector matrix\" or the \"loading matrix.\"\n",
    "\n",
    "**Step 5: Form the Eigenvalue Matrix**\n",
    "\n",
    "Create a diagonal matrix, often denoted as Λ, where the diagonal elements are the ordered eigenvalues.\n",
    "\n",
    "**Step 6: Calculate the Eigendecomposition**\n",
    "\n",
    "The eigendecomposition of the covariance matrix is given by:\n",
    "\n",
    "$$ \\Sigma = PΛP^{-1} $$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\begin{gather}\n",
    "- \\Sigma \\text{ is the covariance matrix. } \\\\\n",
    "- P is the eigenvector matrix. \\\\\n",
    "- \\Lambda \\text { is the eigenvalue matrix. } \\\\\n",
    "- P^{-1} \\text { is the inverse of the eigenvector matrix } \\\\\n",
    "         \\text{    (which is usually the transpose of P if the eigenvectors are orthogonal). }\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "This eigendecomposition represents the covariance matrix as a sum of contributions from its principal components, where Λ contains the variances along these components, and P contains the directions of those components.\n",
    "\n",
    "Eigendecomposition is a crucial step in principal component analysis (PCA) and dimensionality reduction techniques, as it helps identify the most important features or dimensions in your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
